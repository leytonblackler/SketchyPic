{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"train\"\n",
    "# MODE = \"test\"\n",
    "# MODE = \"export\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory where the combined photos and sketch pairs images are located.\n",
    "COMBINED_IMAGES_DIRECTORY = \"../sketchy_database_photo_pairs/\"\n",
    "\n",
    "# The percentage of input image data to allocate for the training dataset, remainder is allocated to test datset.\n",
    "TRAINING_PORTION = 0.9\n",
    "\n",
    "# The directory where files are outputted to.\n",
    "OUTPUT_DIRECTORY = \"../gan_output/\"\n",
    "\n",
    "# The seed to use \"randomisation\".\n",
    "SEED = 10\n",
    "\n",
    "CHECKPOINT_DIRECTORY = \"../gan_checkpoints\"\n",
    "\n",
    "# The number of training steps (0 to disable).\n",
    "MAX_TRAINING_STEPS = 0\n",
    "\n",
    "# The number of training epochs.\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "# How often summaries should be updated in terms of summary_freq steps.\n",
    "SUMMARY_FREQUENCY = 100\n",
    "\n",
    "# How often progress should be displayed in terms of progress_freq steps.\n",
    "PROGRESS_FREQUENCY = 100\n",
    "\n",
    "# How often trace execution should be performed in terms of trace_freq steps.\n",
    "TRACE_FREQUENCY = 100\n",
    "\n",
    "# How often training images should be written in terms of display_freq steps.\n",
    "DISPLAY_FREQUENCY = 100\n",
    "\n",
    "# How often the model should be saved in terms of save_freq steps (0 to disable).\n",
    "SAVE_FREQUENCY = 100\n",
    "\n",
    "# Use separable convolutions in the generator?\n",
    "USE_SEPARABLE_CONVOLUTIONS = True\n",
    "\n",
    "# Split input image into brightness (A) and color (B)?\n",
    "LAB_COLORIZATION = True\n",
    "\n",
    "# Number of images to use in each batch.\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Number of generator filters in first convolutional layer.\n",
    "GENERATOR_FILTERS_IN_FIRST_LAYER = 64\n",
    "\n",
    "# Number of discriminator filters in first convolutional layer.\n",
    "DISCRIMINATOR_FILTERS_IN_FIRST_LAYER = 64\n",
    "\n",
    "# The initial learning rate to use for the Adam adaptive learning rate optimization algorithm.\n",
    "INITIAL_LEARNING_RATE = 0.0002\n",
    "\n",
    "# The momentum turn to use for the Adam adaptive learning rate optimization algorithm.\n",
    "MOMENTUM_TURN = 0.5\n",
    "\n",
    "# The weight on L1 term for generator gradient.\n",
    "L1_WEIGHT = 100.0\n",
    "\n",
    "# The weight on GAN term for generator gradient.\n",
    "GAN_WEIGHT = 1.0\n",
    "\n",
    "# ???\n",
    "EPS = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = collections.namedtuple(\"Examples\", \"paths, inputs, targets, count, steps_per_epoch\")\n",
    "Model = collections.namedtuple(\"Model\", \"outputs, predict_real, predict_fake, discrim_loss, discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1, gen_grads_and_vars, train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    with tf.name_scope(\"preprocess\"):\n",
    "        # [0, 1] => [-1, 1]\n",
    "        return image * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess(image):\n",
    "    with tf.name_scope(\"deprocess\"):\n",
    "        # [-1, 1] => [0, 1]\n",
    "        return (image + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lab(lab):\n",
    "    with tf.name_scope(\"preprocess_lab\"):\n",
    "        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)\n",
    "        # L_chan: black and white with input range [0, 100]\n",
    "        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact\n",
    "        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]\n",
    "        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_lab(L_chan, a_chan, b_chan):\n",
    "    with tf.name_scope(\"deprocess_lab\"):\n",
    "        # this is axis=3 instead of axis=2 because we process individual images but deprocess batches\n",
    "        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, brightness):\n",
    "    # (a, b) color channels, combine with L channel and convert to rgb\n",
    "    a_chan, b_chan = tf.unstack(image, axis=3)\n",
    "    L_chan = tf.squeeze(brightness, axis=3)\n",
    "    lab = deprocess_lab(L_chan, a_chan, b_chan)\n",
    "    rgb = lab_to_rgb(lab)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_conv(batch_input, out_channels, stride):\n",
    "    padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "    return tf.layers.conv2d(padded_input, out_channels, kernel_size=4, strides=(stride, stride), padding=\"valid\", kernel_initializer=tf.random_normal_initializer(0, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_conv(batch_input, out_channels):\n",
    "    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    if a.separable_conv:\n",
    "        return tf.layers.separable_conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", depthwise_initializer=initializer, pointwise_initializer=initializer)\n",
    "    else:\n",
    "        return tf.layers.conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", kernel_initializer=initializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_deconv(batch_input, out_channels):\n",
    "    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    if a.separable_conv:\n",
    "        _b, h, w, _c = batch_input.shape\n",
    "        resized_input = tf.image.resize_images(batch_input, [h * 2, w * 2], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        return tf.layers.separable_conv2d(resized_input, out_channels, kernel_size=4, strides=(1, 1), padding=\"same\", depthwise_initializer=initializer, pointwise_initializer=initializer)\n",
    "    else:\n",
    "        return tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", kernel_initializer=initializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, a):\n",
    "    with tf.name_scope(\"lrelu\"):\n",
    "        # adding these together creates the leak part and linear part\n",
    "        # then cancels them out by subtracting/adding an absolute value term\n",
    "        # leak: a*x/2 - a*abs(x)/2\n",
    "        # linear: x/2 + abs(x)/2\n",
    "\n",
    "        # this block looks like it has 2 inputs on the graph unless we do this\n",
    "        x = tf.identity(x)\n",
    "        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm(inputs):\n",
    "    return tf.layers.batch_normalization(inputs, axis=3, epsilon=1e-5, momentum=0.1, training=True, gamma_initializer=tf.random_normal_initializer(1.0, 0.02))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(image):\n",
    "    assertion = tf.assert_equal(tf.shape(image)[-1], 3, message=\"image must have 3 color channels\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "        image = tf.identity(image)\n",
    "\n",
    "    if image.get_shape().ndims not in (3, 4):\n",
    "        raise ValueError(\"image must be either 3 or 4 dimensions\")\n",
    "\n",
    "    # make the last dimension 3 so that you can unstack the colors\n",
    "    shape = list(image.get_shape())\n",
    "    shape[-1] = 3\n",
    "    image.set_shape(shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\n",
    "def rgb_to_lab(srgb):\n",
    "    with tf.name_scope(\"rgb_to_lab\"):\n",
    "        srgb = check_image(srgb)\n",
    "        srgb_pixels = tf.reshape(srgb, [-1, 3])\n",
    "\n",
    "        with tf.name_scope(\"srgb_to_xyz\"):\n",
    "            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n",
    "            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n",
    "            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n",
    "            rgb_to_xyz = tf.constant([\n",
    "                #    X        Y          Z\n",
    "                [0.412453, 0.212671, 0.019334], # R\n",
    "                [0.357580, 0.715160, 0.119193], # G\n",
    "                [0.180423, 0.072169, 0.950227], # B\n",
    "            ])\n",
    "            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n",
    "\n",
    "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
    "        with tf.name_scope(\"xyz_to_cielab\"):\n",
    "            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)\n",
    "\n",
    "            # normalize for D65 white point\n",
    "            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1/0.950456, 1.0, 1/1.088754])\n",
    "\n",
    "            epsilon = 6/29\n",
    "            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n",
    "            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n",
    "            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4/29) * linear_mask + (xyz_normalized_pixels ** (1/3)) * exponential_mask\n",
    "\n",
    "            # convert to lab\n",
    "            fxfyfz_to_lab = tf.constant([\n",
    "                #  l       a       b\n",
    "                [  0.0,  500.0,    0.0], # fx\n",
    "                [116.0, -500.0,  200.0], # fy\n",
    "                [  0.0,    0.0, -200.0], # fz\n",
    "            ])\n",
    "            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n",
    "\n",
    "        return tf.reshape(lab_pixels, tf.shape(srgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb(lab):\n",
    "    with tf.name_scope(\"lab_to_rgb\"):\n",
    "        lab = check_image(lab)\n",
    "        lab_pixels = tf.reshape(lab, [-1, 3])\n",
    "\n",
    "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
    "        with tf.name_scope(\"cielab_to_xyz\"):\n",
    "            # convert to fxfyfz\n",
    "            lab_to_fxfyfz = tf.constant([\n",
    "                #   fx      fy        fz\n",
    "                [1/116.0, 1/116.0,  1/116.0], # l\n",
    "                [1/500.0,     0.0,      0.0], # a\n",
    "                [    0.0,     0.0, -1/200.0], # b\n",
    "            ])\n",
    "            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n",
    "\n",
    "            # convert to xyz\n",
    "            epsilon = 6/29\n",
    "            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n",
    "            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n",
    "            xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n",
    "\n",
    "            # denormalize for D65 white point\n",
    "            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n",
    "\n",
    "        with tf.name_scope(\"xyz_to_srgb\"):\n",
    "            xyz_to_rgb = tf.constant([\n",
    "                #     r           g          b\n",
    "                [ 3.2404542, -0.9692660,  0.0556434], # x\n",
    "                [-1.5371385,  1.8760108, -0.2040259], # y\n",
    "                [-0.4985314,  0.0415560,  1.0572252], # z\n",
    "            ])\n",
    "            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n",
    "            # avoid a slightly negative number messing up the conversion\n",
    "            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n",
    "            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n",
    "            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n",
    "            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1/2.4) * 1.055) - 0.055) * exponential_mask\n",
    "\n",
    "        return tf.reshape(srgb_pixels, tf.shape(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples():\n",
    "    if COMBINED_IMAGES_DIRECTORY is None or not os.path.exists(COMBINED_IMAGES_DIRECTORY):\n",
    "        raise Exception(\"input_dir does not exist\")\n",
    "\n",
    "    input_paths = glob.glob(os.path.join(COMBINED_IMAGES_DIRECTORY, \"*.jpg\"))\n",
    "    decode = tf.image.decode_jpeg\n",
    "    if len(input_paths) == 0:\n",
    "        input_paths = glob.glob(os.path.join(COMBINED_IMAGES_DIRECTORY, \"*.png\"))\n",
    "        decode = tf.image.decode_png\n",
    "\n",
    "    if len(input_paths) == 0:\n",
    "        raise Exception(\"input_dir contains no image files\")\n",
    "\n",
    "    def get_name(path):\n",
    "        name, _ = os.path.splitext(os.path.basename(path))\n",
    "        return name\n",
    "\n",
    "    # if the image names are numbers, sort by the value rather than asciibetically\n",
    "    # having sorted inputs means that the outputs are sorted in test mode\n",
    "    if all(get_name(path).isdigit() for path in input_paths):\n",
    "        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\n",
    "    else:\n",
    "        input_paths = sorted(input_paths)\n",
    "\n",
    "    with tf.name_scope(\"load_images\"):\n",
    "        path_queue = tf.train.string_input_producer(input_paths, shuffle=MODE == \"train\")\n",
    "        reader = tf.WholeFileReader()\n",
    "        paths, contents = reader.read(path_queue)\n",
    "        raw_input = decode(contents)\n",
    "        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)\n",
    "\n",
    "        assertion = tf.assert_equal(tf.shape(raw_input)[2], 3, message=\"image does not have 3 channels\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            raw_input = tf.identity(raw_input)\n",
    "\n",
    "        raw_input.set_shape([None, None, 3])\n",
    "\n",
    "        if a.lab_colorization:\n",
    "            # load color and brightness from image, no B image exists here\n",
    "            lab = rgb_to_lab(raw_input)\n",
    "            L_chan, a_chan, b_chan = preprocess_lab(lab)\n",
    "            a_images = tf.expand_dims(L_chan, axis=2)\n",
    "            b_images = tf.stack([a_chan, b_chan], axis=2)\n",
    "        else:\n",
    "            # break apart image pair and move to range [-1, 1]\n",
    "            width = tf.shape(raw_input)[1] # [height, width, channels]\n",
    "            a_images = preprocess(raw_input[:,:width//2,:])\n",
    "            b_images = preprocess(raw_input[:,width//2:,:])\n",
    "\n",
    "    if a.which_direction == \"AtoB\":\n",
    "        inputs, targets = [a_images, b_images]\n",
    "    elif a.which_direction == \"BtoA\":\n",
    "        inputs, targets = [b_images, a_images]\n",
    "    else:\n",
    "        raise Exception(\"invalid direction\")\n",
    "\n",
    "    # synchronize seed for image operations so that we do the same operations to both\n",
    "    # input and output images\n",
    "    seed = random.randint(0, 2**31 - 1)\n",
    "    def transform(image):\n",
    "        r = image\n",
    "\n",
    "        # area produces a nice downscaling, but does nearest neighbor for upscaling\n",
    "        # assume we're going to be doing downscaling here\n",
    "        r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)\n",
    "\n",
    "        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, a.scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\n",
    "        if a.scale_size > CROP_SIZE:\n",
    "            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\n",
    "        elif a.scale_size < CROP_SIZE:\n",
    "            raise Exception(\"scale size cannot be less than crop size\")\n",
    "        return r\n",
    "\n",
    "    with tf.name_scope(\"input_images\"):\n",
    "        input_images = transform(inputs)\n",
    "\n",
    "    with tf.name_scope(\"target_images\"):\n",
    "        target_images = transform(targets)\n",
    "\n",
    "    paths_batch, inputs_batch, targets_batch = tf.train.batch([paths, input_images, target_images], batch_size=a.batch_size)\n",
    "    steps_per_epoch = int(math.ceil(len(input_paths) / a.batch_size))\n",
    "\n",
    "    return Examples(\n",
    "        paths=paths_batch,\n",
    "        inputs=inputs_batch,\n",
    "        targets=targets_batch,\n",
    "        count=len(input_paths),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(generator_inputs, generator_outputs_channels):\n",
    "    layers = []\n",
    "\n",
    "    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\n",
    "    with tf.variable_scope(\"encoder_1\"):\n",
    "        output = gen_conv(generator_inputs, a.ngf)\n",
    "        layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        a.ngf * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
    "        a.ngf * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
    "        a.ngf * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
    "        a.ngf * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
    "        a.ngf * 8, # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
    "        a.ngf * 8, # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
    "        a.ngf * 8, # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
    "    ]\n",
    "\n",
    "    for out_channels in layer_specs:\n",
    "        with tf.variable_scope(\"encoder_%d\" % (len(layers) + 1)):\n",
    "            rectified = lrelu(layers[-1], 0.2)\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n",
    "            convolved = gen_conv(rectified, out_channels)\n",
    "            output = batchnorm(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        (a.ngf * 8, 0.5),   # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n",
    "        (a.ngf * 8, 0.5),   # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n",
    "        (a.ngf * 8, 0.5),   # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n",
    "        (a.ngf * 8, 0.0),   # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n",
    "        (a.ngf * 4, 0.0),   # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
    "        (a.ngf * 2, 0.0),   # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
    "        (a.ngf, 0.0),       # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
    "    ]\n",
    "\n",
    "    num_encoder_layers = len(layers)\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "        with tf.variable_scope(\"decoder_%d\" % (skip_layer + 1)):\n",
    "            if decoder_layer == 0:\n",
    "                # first decoder layer doesn't have skip connections\n",
    "                # since it is directly connected to the skip_layer\n",
    "                input = layers[-1]\n",
    "            else:\n",
    "                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "            rectified = tf.nn.relu(input)\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n",
    "            output = gen_deconv(rectified, out_channels)\n",
    "            output = batchnorm(output)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\n",
    "    with tf.variable_scope(\"decoder_1\"):\n",
    "        input = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "        rectified = tf.nn.relu(input)\n",
    "        output = gen_deconv(rectified, generator_outputs_channels)\n",
    "        output = tf.tanh(output)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, targets):\n",
    "    def create_discriminator(discrim_inputs, discrim_targets):\n",
    "        n_layers = 3\n",
    "        layers = []\n",
    "\n",
    "        # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\n",
    "        input = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
    "\n",
    "        # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\n",
    "        with tf.variable_scope(\"layer_1\"):\n",
    "            convolved = discrim_conv(input, a.ndf, stride=2)\n",
    "            rectified = lrelu(convolved, 0.2)\n",
    "            layers.append(rectified)\n",
    "\n",
    "        # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "        # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "        # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "        for i in range(n_layers):\n",
    "            with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
    "                out_channels = a.ndf * min(2**(i+1), 8)\n",
    "                stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\n",
    "                convolved = discrim_conv(layers[-1], out_channels, stride=stride)\n",
    "                normalized = batchnorm(convolved)\n",
    "                rectified = lrelu(normalized, 0.2)\n",
    "                layers.append(rectified)\n",
    "\n",
    "        # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n",
    "        with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
    "            convolved = discrim_conv(rectified, out_channels=1, stride=1)\n",
    "            output = tf.sigmoid(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "        return layers[-1]\n",
    "\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        out_channels = int(targets.get_shape()[-1])\n",
    "        outputs = create_generator(inputs, out_channels)\n",
    "\n",
    "    # create two copies of discriminator, one for real pairs and one for fake pairs\n",
    "    # they share the same underlying variables\n",
    "    with tf.name_scope(\"real_discriminator\"):\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
    "            predict_real = create_discriminator(inputs, targets)\n",
    "\n",
    "    with tf.name_scope(\"fake_discriminator\"):\n",
    "        with tf.variable_scope(\"discriminator\", reuse=True):\n",
    "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
    "            predict_fake = create_discriminator(inputs, outputs)\n",
    "\n",
    "    with tf.name_scope(\"discriminator_loss\"):\n",
    "        # minimizing -tf.log will try to get inputs to 1\n",
    "        # predict_real => 1\n",
    "        # predict_fake => 0\n",
    "        discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))\n",
    "\n",
    "    with tf.name_scope(\"generator_loss\"):\n",
    "        # predict_fake => 1\n",
    "        # abs(targets - outputs) => 0\n",
    "        gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake + EPS))\n",
    "        gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
    "        gen_loss = gen_loss_GAN * a.gan_weight + gen_loss_L1 * a.l1_weight\n",
    "\n",
    "    with tf.name_scope(\"discriminator_train\"):\n",
    "        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
    "        discrim_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n",
    "        discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\n",
    "        discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\n",
    "\n",
    "    with tf.name_scope(\"generator_train\"):\n",
    "        with tf.control_dependencies([discrim_train]):\n",
    "            gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
    "            gen_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n",
    "            gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\n",
    "            gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    update_losses = ema.apply([discrim_loss, gen_loss_GAN, gen_loss_L1])\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    incr_global_step = tf.assign(global_step, global_step+1)\n",
    "\n",
    "    return Model(\n",
    "        predict_real=predict_real,\n",
    "        predict_fake=predict_fake,\n",
    "        discrim_loss=ema.average(discrim_loss),\n",
    "        discrim_grads_and_vars=discrim_grads_and_vars,\n",
    "        gen_loss_GAN=ema.average(gen_loss_GAN),\n",
    "        gen_loss_L1=ema.average(gen_loss_L1),\n",
    "        gen_grads_and_vars=gen_grads_and_vars,\n",
    "        outputs=outputs,\n",
    "        train=tf.group(update_losses, incr_global_step, gen_train),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(fetches, step=None):\n",
    "    image_dir = os.path.join(OUTPUT_DIRECTORY, \"images\")\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    filesets = []\n",
    "    for i, in_path in enumerate(fetches[\"paths\"]):\n",
    "        name, _ = os.path.splitext(os.path.basename(in_path.decode(\"utf8\")))\n",
    "        fileset = {\"name\": name, \"step\": step}\n",
    "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
    "            filename = name + \"-\" + kind + \".png\"\n",
    "            if step is not None:\n",
    "                filename = \"%08d-%s\" % (step, filename)\n",
    "            fileset[kind] = filename\n",
    "            out_path = os.path.join(image_dir, filename)\n",
    "            contents = fetches[kind][i]\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(contents)\n",
    "        filesets.append(fileset)\n",
    "    return filesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_index(filesets, step=False):\n",
    "    index_path = os.path.join(OUTPUT_DIRECTORY, \"index.html\")\n",
    "    if os.path.exists(index_path):\n",
    "        index = open(index_path, \"a\")\n",
    "    else:\n",
    "        index = open(index_path, \"w\")\n",
    "        index.write(\"<html><body><table><tr>\")\n",
    "        if step:\n",
    "            index.write(\"<th>step</th>\")\n",
    "        index.write(\"<th>name</th><th>input</th><th>output</th><th>target</th></tr>\")\n",
    "\n",
    "    for fileset in filesets:\n",
    "        index.write(\"<tr>\")\n",
    "\n",
    "        if step:\n",
    "            index.write(\"<td>%d</td>\" % fileset[\"step\"])\n",
    "        index.write(\"<td>%s</td>\" % fileset[\"name\"])\n",
    "\n",
    "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
    "            index.write(\"<td><img src='images/%s'></td>\" % fileset[kind])\n",
    "\n",
    "        index.write(\"</tr>\")\n",
    "    return index_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     tf.set_random_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "\n",
    "    if MODE == \"test\" or MODE == \"export\":\n",
    "        if CHECKPOINT_DIRECTORY is None:\n",
    "            raise Exception(\"checkpoint required for test mode\")\n",
    "\n",
    "        # load some options from the checkpoint\n",
    "        options = {\"which_direction\", \"ngf\", \"ndf\", \"lab_colorization\"}\n",
    "        with open(os.path.join(a.checkpoint, \"options.json\")) as f:\n",
    "            for key, val in json.loads(f.read()).items():\n",
    "                if key in options:\n",
    "                    print(\"loaded\", key, \"=\", val)\n",
    "                    setattr(a, key, val)\n",
    "        # disable these features in test mode\n",
    "        a.scale_size = CROP_SIZE\n",
    "\n",
    "    if MODE == \"export\":\n",
    "        # export the generator to a meta graph that can be imported later for standalone generation\n",
    "        if a.lab_colorization:\n",
    "            raise Exception(\"export not supported for lab_colorization\")\n",
    "\n",
    "        input = tf.placeholder(tf.string, shape=[1])\n",
    "        input_data = tf.decode_base64(input[0])\n",
    "        input_image = tf.image.decode_png(input_data)\n",
    "\n",
    "        # remove alpha channel if present\n",
    "        input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 4), lambda: input_image[:,:,:3], lambda: input_image)\n",
    "        # convert grayscale to RGB\n",
    "        input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 1), lambda: tf.image.grayscale_to_rgb(input_image), lambda: input_image)\n",
    "\n",
    "        input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
    "        input_image.set_shape([CROP_SIZE, CROP_SIZE, 3])\n",
    "        batch_input = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        with tf.variable_scope(\"generator\"):\n",
    "            batch_output = deprocess(create_generator(preprocess(batch_input), 3))\n",
    "\n",
    "        output_image = tf.image.convert_image_dtype(batch_output, dtype=tf.uint8)[0]\n",
    "        if a.output_filetype == \"png\":\n",
    "            output_data = tf.image.encode_png(output_image)\n",
    "        elif a.output_filetype == \"jpeg\":\n",
    "            output_data = tf.image.encode_jpeg(output_image, quality=80)\n",
    "        else:\n",
    "            raise Exception(\"invalid filetype\")\n",
    "        output = tf.convert_to_tensor([tf.encode_base64(output_data)])\n",
    "\n",
    "        key = tf.placeholder(tf.string, shape=[1])\n",
    "        inputs = {\n",
    "            \"key\": key.name,\n",
    "            \"input\": input.name\n",
    "        }\n",
    "        tf.add_to_collection(\"inputs\", json.dumps(inputs))\n",
    "        outputs = {\n",
    "            \"key\":  tf.identity(key).name,\n",
    "            \"output\": output.name,\n",
    "        }\n",
    "        tf.add_to_collection(\"outputs\", json.dumps(outputs))\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        restore_saver = tf.train.Saver()\n",
    "        export_saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            print(\"loading model from checkpoint\")\n",
    "            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\n",
    "            restore_saver.restore(sess, checkpoint)\n",
    "            print(\"exporting model\")\n",
    "            export_saver.export_meta_graph(filename=os.path.join(OUTPUT_DIRECTORY, \"export.meta\"))\n",
    "            export_saver.save(sess, os.path.join(OUTPUT_DIRECTORY, \"export\"), write_meta_graph=False)\n",
    "\n",
    "        return\n",
    "\n",
    "    examples = load_examples()\n",
    "    print(\"examples count = %d\" % examples.count)\n",
    "\n",
    "    # inputs and targets are [batch_size, height, width, channels]\n",
    "    model = create_model(examples.inputs, examples.targets)\n",
    "\n",
    "    # undo colorization splitting on images that we use for display/output\n",
    "    if a.lab_colorization:\n",
    "        if a.which_direction == \"AtoB\":\n",
    "            # inputs is brightness, this will be handled fine as a grayscale image\n",
    "            # need to augment targets and outputs with brightness\n",
    "            targets = augment(examples.targets, examples.inputs)\n",
    "            outputs = augment(model.outputs, examples.inputs)\n",
    "            # inputs can be deprocessed normally and handled as if they are single channel\n",
    "            # grayscale images\n",
    "            inputs = deprocess(examples.inputs)\n",
    "        elif a.which_direction == \"BtoA\":\n",
    "            # inputs will be color channels only, get brightness from targets\n",
    "            inputs = augment(examples.inputs, examples.targets)\n",
    "            targets = deprocess(examples.targets)\n",
    "            outputs = deprocess(model.outputs)\n",
    "        else:\n",
    "            raise Exception(\"invalid direction\")\n",
    "    else:\n",
    "        inputs = deprocess(examples.inputs)\n",
    "        targets = deprocess(examples.targets)\n",
    "        outputs = deprocess(model.outputs)\n",
    "\n",
    "    def convert(image):\n",
    "        if a.aspect_ratio != 1.0:\n",
    "            # upscale to correct aspect ratio\n",
    "            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]\n",
    "            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\n",
    "\n",
    "    # reverse any processing on images so they can be written to disk or displayed to user\n",
    "    with tf.name_scope(\"convert_inputs\"):\n",
    "        converted_inputs = convert(inputs)\n",
    "\n",
    "    with tf.name_scope(\"convert_targets\"):\n",
    "        converted_targets = convert(targets)\n",
    "\n",
    "    with tf.name_scope(\"convert_outputs\"):\n",
    "        converted_outputs = convert(outputs)\n",
    "\n",
    "    with tf.name_scope(\"encode_images\"):\n",
    "        display_fetches = {\n",
    "            \"paths\": examples.paths,\n",
    "            \"inputs\": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\"input_pngs\"),\n",
    "            \"targets\": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=\"target_pngs\"),\n",
    "            \"outputs\": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\"output_pngs\"),\n",
    "        }\n",
    "\n",
    "    # summaries\n",
    "    with tf.name_scope(\"inputs_summary\"):\n",
    "        tf.summary.image(\"inputs\", converted_inputs)\n",
    "\n",
    "    with tf.name_scope(\"targets_summary\"):\n",
    "        tf.summary.image(\"targets\", converted_targets)\n",
    "\n",
    "    with tf.name_scope(\"outputs_summary\"):\n",
    "        tf.summary.image(\"outputs\", converted_outputs)\n",
    "\n",
    "    with tf.name_scope(\"predict_real_summary\"):\n",
    "        tf.summary.image(\"predict_real\", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\n",
    "\n",
    "    with tf.name_scope(\"predict_fake_summary\"):\n",
    "        tf.summary.image(\"predict_fake\", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\n",
    "\n",
    "    tf.summary.scalar(\"discriminator_loss\", model.discrim_loss)\n",
    "    tf.summary.scalar(\"generator_loss_GAN\", model.gen_loss_GAN)\n",
    "    tf.summary.scalar(\"generator_loss_L1\", model.gen_loss_L1)\n",
    "\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name + \"/values\", var)\n",
    "\n",
    "    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\n",
    "        tf.summary.histogram(var.op.name + \"/gradients\", grad)\n",
    "\n",
    "    with tf.name_scope(\"parameter_count\"):\n",
    "        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    logdir = OUTPUT_DIRECTORY if (a.trace_freq > 0 or a.summary_freq > 0) else None\n",
    "    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\n",
    "    with sv.managed_session() as sess:\n",
    "        print(\"parameter_count =\", sess.run(parameter_count))\n",
    "\n",
    "        if a.checkpoint is not None:\n",
    "            print(\"loading model from checkpoint\")\n",
    "            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "        max_steps = 2**32\n",
    "        if a.max_epochs is not None:\n",
    "            max_steps = examples.steps_per_epoch * a.max_epochs\n",
    "        if a.max_steps is not None:\n",
    "            max_steps = a.max_steps\n",
    "\n",
    "        if MODE == \"test\":\n",
    "            # testing\n",
    "            # at most, process the test data once\n",
    "            start = time.time()\n",
    "            max_steps = min(examples.steps_per_epoch, max_steps)\n",
    "            for step in range(max_steps):\n",
    "                results = sess.run(display_fetches)\n",
    "                filesets = save_images(results)\n",
    "                for i, f in enumerate(filesets):\n",
    "                    print(\"evaluated image\", f[\"name\"])\n",
    "                index_path = append_index(filesets)\n",
    "            print(\"wrote index at\", index_path)\n",
    "            print(\"rate\", (time.time() - start) / max_steps)\n",
    "        else:\n",
    "            # training\n",
    "            start = time.time()\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                def should(freq):\n",
    "                    return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\n",
    "\n",
    "                options = None\n",
    "                run_metadata = None\n",
    "                if should(a.trace_freq):\n",
    "                    options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "\n",
    "                fetches = {\n",
    "                    \"train\": model.train,\n",
    "                    \"global_step\": sv.global_step,\n",
    "                }\n",
    "\n",
    "                if should(a.progress_freq):\n",
    "                    fetches[\"discrim_loss\"] = model.discrim_loss\n",
    "                    fetches[\"gen_loss_GAN\"] = model.gen_loss_GAN\n",
    "                    fetches[\"gen_loss_L1\"] = model.gen_loss_L1\n",
    "\n",
    "                if should(a.summary_freq):\n",
    "                    fetches[\"summary\"] = sv.summary_op\n",
    "\n",
    "                if should(a.display_freq):\n",
    "                    fetches[\"display\"] = display_fetches\n",
    "\n",
    "                results = sess.run(fetches, options=options, run_metadata=run_metadata)\n",
    "\n",
    "                if should(a.summary_freq):\n",
    "                    print(\"recording summary\")\n",
    "                    sv.summary_writer.add_summary(results[\"summary\"], results[\"global_step\"])\n",
    "\n",
    "                if should(a.display_freq):\n",
    "                    print(\"saving display images\")\n",
    "                    filesets = save_images(results[\"display\"], step=results[\"global_step\"])\n",
    "                    append_index(filesets, step=True)\n",
    "\n",
    "                if should(a.trace_freq):\n",
    "                    print(\"recording trace\")\n",
    "                    sv.summary_writer.add_run_metadata(run_metadata, \"step_%d\" % results[\"global_step\"])\n",
    "\n",
    "                if should(a.progress_freq):\n",
    "                    # global_step will have the correct step count if we resume from a checkpoint\n",
    "                    train_epoch = math.ceil(results[\"global_step\"] / examples.steps_per_epoch)\n",
    "                    train_step = (results[\"global_step\"] - 1) % examples.steps_per_epoch + 1\n",
    "                    rate = (step + 1) * a.batch_size / (time.time() - start)\n",
    "                    remaining = (max_steps - step) * a.batch_size / rate\n",
    "                    print(\"progress  epoch %d  step %d  image/sec %0.1f  remaining %dm\" % (train_epoch, train_step, rate, remaining / 60))\n",
    "                    print(\"discrim_loss\", results[\"discrim_loss\"])\n",
    "                    print(\"gen_loss_GAN\", results[\"gen_loss_GAN\"])\n",
    "                    print(\"gen_loss_L1\", results[\"gen_loss_L1\"])\n",
    "\n",
    "                if should(a.save_freq):\n",
    "                    print(\"saving model\")\n",
    "                    saver.save(sess, os.path.join(OUTPUT_DIRECTORY, \"model\"), global_step=sv.global_step)\n",
    "\n",
    "                if sv.should_stop():\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'string_input_producer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-b3d3686a2962>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"examples count = %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-fbbefdc40ac6>\u001b[0m in \u001b[0;36mload_examples\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpath_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_input_producer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWholeFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'string_input_producer'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
